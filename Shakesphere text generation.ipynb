{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_md',disable = ['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the roman characters\n",
    "# with open('shakespeare.txt','r') as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "# with open('sheakesphere_noroman.txt','w') as f:\n",
    "#     raw_text_without_roman_int = re.sub(\"[IVXLCDM]+\",'\\n',raw_text)\n",
    "#     f.write(raw_text_without_roman_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in files as a string text\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        text_data = f.read()\n",
    "    return text_data\n",
    "\n",
    "word_doc = read_file('sheakesphere_noroman.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text and Tokenize\n",
    "def remove_punc(word_doc):\n",
    "    tokens = [token.text.lower() for token in nlp(word_doc) if token.text not in '\\n\\n\\n\\n  \\n\\n\\n  \\n    \\n\\n  \\n\\n\\n  \\n    \\n    \\n\\n\\n\\n  \\n\\n\\n  \\n\\n  \\n\\n\\n  \\n\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']\n",
    "    return tokens\n",
    "\n",
    "tokens = remove_punc(word_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[56:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sequences of Tokens\n",
    "def token_sequence(tokens,train_len):\n",
    "    sequences = []\n",
    "    for i in range(train_len,len(tokens)):\n",
    "        seq = tokens[i - train_len:i]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "train_len = 25 + 1   \n",
    "text_sequences = token_sequence(tokens,train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Numpy Matrix\n",
    "import numpy as np\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an LSTM based model function\n",
    "def create_model(vocabulary_size,seq_len):\n",
    "    opt = Adam()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size,25,input_length = seq_len))\n",
    "    model.add(LSTM(150,return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150,activation='relu'))\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "seq_len = X.shape[1]\n",
    "\n",
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "early_stoping = EarlyStopping(monitor='loss',patience=15,restore_best_weights=True,mode='min')\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "      if epoch < 220:\n",
    "        return lr\n",
    "      else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./tmp/checkpoint',\n",
    "                                            save_weights_only=True,\n",
    "                                            monitor='loss',\n",
    "                                            mode='min',\n",
    "                                            save_best_only=True)\n",
    "                    \n",
    "callback_list = [early_stoping,learning_rate_scheduler,model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 25)            80100     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 150)           105600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3204)              483804    \n",
      "=================================================================\n",
      "Total params: 872,754\n",
      "Trainable params: 872,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = create_model(vocabulary_size+1,seq_len)\n",
    "# history = model.fit(X,y,callbacks=callback_list,epochs=300,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "from pickle import dump,load\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('epoch300colab.h5',compile=False)\n",
    "tokenizer = load(open('tokenizer300','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating New Text\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len,truncating='pre')\n",
    "        \n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "                \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "               \n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ind = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[rand_ind]\n",
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"arise you live in this and dwell in lovers ' eyes sweet longer part in my deeds breast but all this sort as thou particulars not shall mine eyes for eyes have done ine say thy praise and most idol show since wherein like her\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = generate_text(model,tokenizer,seq_len,seed_text,45)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = '''feeling but by others ' seeing for thou art \\n true ike for the swallow'd bait \\n on purpose laid and rude oubting the tillage \\n of thy brand for almost change \\n my poverty and yet those cause yet few ' grew a prey'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text2 = '''arise you live in this and dwell in lovers \\n ' eyes sweet longer part in my deeds \\n breast but all this sort as thou particulars \\n not shall mine eyes for eyes \\n have done ine say thy praise and most idol show\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeling but by others ' seeing for thou art \n",
      " true ike for the swallow'd bait \n",
      " on purpose laid and rude oubting the tillage \n",
      " of thy brand for almost change \n",
      " my poverty and yet those cause yet few ' grew a prey\n"
     ]
    }
   ],
   "source": [
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arise you live in this and dwell in lovers \n",
      " ' eyes sweet longer part in my deeds \n",
      " breast but all this sort as thou particulars \n",
      " not shall mine eyes for eyes \n",
      " have done ine say thy praise and most idol show\"\n"
     ]
    }
   ],
   "source": [
    "print(gen_text2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
